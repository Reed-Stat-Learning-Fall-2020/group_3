---
title: "Technical Report"
author: "Shisham Adhikari, Maggie Slein, Grayson White"
bibliography: references.bib
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(out.width = "75%")
knitr::opts_chunk$set(fig.align = "center")
library(tidyverse)
theme_set(theme_bw())
```
# Abstract

As American politics has become increasingly polarized over the last several decades, predicting the likelihood of presidential victories has become more difficult. With just a few states' electoral votes deciding the figurehead of the executive branch, two out of the four last elections have been decided based on the electoral college and not on the popular vote. Given this, many previous models used to predict election outcomes have failed to predict the true winner. We aim to address this by using `tinymodels` to layer multiple models and model types to better predict party affiliation using several predictors from the General Social Survey (GSS) dataset from 2000-2016. 

# Introduction

**An overview of the topic and relevant background information, a discussion of existing theories and models, a description of how your investigation differs from prior ones, and a precise statement of your research question.**

In the last two decades, the polarization of American politics has yielded unexpected victories by minority political groups, both in the 2000 and 2016 election. The Republican party represents less Americans by volume than both the Democratic party and the Independent parties, yet has continued to secure political power as a result of the electoral college. The two unprecedented victories in 2000 and 2016 by candidates who lost the popular vote but won the presidency as a result of the electoral college have been attributed to highly variable party affiliation in what have been dubbed "swing states". In these states, the split of Democratic to Republican votes is fairly even, which makes winning their popular votes crucial for ultimately securing their electoral votes, in the electoral college's "winner takes all" framework. These states have been a source of contention in 2 of the 4 elections. 

In 2000, the race between Governor George W. Bush vs. Vice President Al Gore  for the presidency came down to one state: Florida. Gore had secured the popular vote, but he needed to win Florida to win the necessary electoral votes to secure the presidency. Early on during election night, Bush was called as the winner with margin of 100,000 votes. However, as votes from highly democratic districts poured in, the margin between Bush and Gore narrowed to just 2,000 votes. The vote counts in Florida were so close, the law demanded a recount. However, the legally mandated recount was to be performed by machine, not by hand. At the time, some counties used punch ballots and there was concern over the anomalies present in ballots cast with such a narrow margin between the two candidates. Gore pushed to have hand-counted recounts in particular counties via litigation. With the saga of who would ultimately become president-elect enduring for over a month after election day, the Supreme Court Case, Bush v. Gore ended the recount on December 12th, 2000. The verdict in that case was essentially that Gore did not have the grounds to request anything beyond a machine-automated recount of the votes. Thus, the electoral votes from the state of Florida ultimately decided that outcome of the 2000 election: President-elect Governor George W. Bush. 

For the fifth time in US history, the 2016 race between businessman Donald J. Trump and former Secretary of State Hilary Clinton was also decided by the electoral college and not the popular vote. Less contentious than the 2000 election, the results of the 2016 election have been attributed to a lack of concern for winning electoral votes in key swing states. As these highly influential and dynamic "swing states" shift with each passing election, a better of understanding and prediction of their political party tendencies becomes increasing important. To better understand how external factors drive political party affiliation and ultimately predict political party affiliation, we are interested using techniques that combine many model types to provide more accurate conclusions.

Previous models have utilized one statistical learning method ()


Research Question: *How can we use a combination of models to predict political party affiliation using small sample sizes?*

Our group wishes to understand the external factors related to political party affiilation, and to understand how well we can predict political party affiliation by using a combination of the modeling techniques learned in class. 



# Methods


# Data 

## The GSS dataset

The general social survey (GSS) is a massive survey conducted of people within the United States since 1972. The GSS aims to get a representative sample of people in the United States and to understand information about them and how they feel about social and political issues. We have chosen some key variables collected from this survey, along with participants from 2000 or more recent, in order for us to attempt to classify political affiliation of participants. Our subset of the GSS dataset contains 5,800 rows, 16 columns, and 0 NA's.  

## Filtering

Most of this filtering was done for the `infer` package `gss` dataset and can be attributed to authors of that package. We have included more rows and columns than that package, however, much initial tidying and subsetting can be attributed to them [@infer]. Below is the code adapted from the `infer` package to attain our dataset, `gss_subset`:
```{r gss_filter, message = FALSE, eval = FALSE}
load("gss/gss_orig.rda")
gss_subset <- gss_orig %>%
  filter(!stringr::str_detect(sample, "blk oversamp")) %>% # this is for weighting
  select(year, age, sex, college = degree, partyid, hompop, hours = hrs1, income,
         class, finrela, wrkgovt, marital, educ, race, incom16, weight = wtssall) %>%
  mutate_if(is.factor, ~ fct_collapse(., NULL = c("IAP", "NA", "iap", "na"))) %>%
  mutate(
    age = age %>%
      fct_recode("89" = "89 or older",
                 NULL = "DK") %>%
      as.character() %>%
      as.numeric(),
    hompop = hompop %>%
      fct_collapse(NULL = c("DK")) %>%
      as.character() %>%
      as.numeric(),
    hours = hours %>%
      fct_recode("89" = "89+ hrs",
                 NULL = "DK") %>%
      as.character() %>%
      as.numeric(),
    weight = weight %>%
      as.character() %>%
      as.numeric(),
    partyid = fct_collapse(
      partyid,
      dem = c("strong democrat", "not str democrat"),
      rep = c("strong republican", "not str republican"),
      ind = c("ind,near dem", "independent", "ind,near rep"),
      other = "other party"
    ),
    income = factor(income, ordered = TRUE),
    college = fct_collapse(
      college,
      degree = c("junior college", "bachelor", "graduate"),
      "no degree"  = c("lt high school", "high school"),
      NULL = "dk"
    )
  ) %>%
  filter(year >= 2000) %>%
  filter(partyid %in% c("dem", "rep")) %>%
  drop_na()
```
```{r actually_load_gss, echo = FALSE}
# run this chunk to quickly load the gss subset without having to re-run the filtering
load("gss/gss_subset.rda")
```

Given our goal to understand which factors influence party affiliation in the US, we selected `year` (year of the election), `age` (age of time of survey), `college` (degree or no degree), `partyid` (democrat or republican), `hompop` (number of people in the respondent's household), `hours` (number of hours worked in the last week), `income` (total family income, categorical), `class` (socioeconomic class as described by respondent), `finrela` (respondent's opinion on family's income level), `wrkgovt` (whether or not the respondent works for the government), `marital` (respondent's martial status), `educ` (highest year of school completed), `race` (race of respondent), `income16` (respondent's family income at the age of 16), and `weight` (survey weight). 

We made some choices while filtering the dataset which will effect the final results of our models. First of all, we have filtered all observations which do not state that their political affiliation was either democrat or republican. We are most interested in answering the question of whether or not we can classify between these parties rather than considering much smaller third parties. Also, we have filtered all observations with any NA's. We chose to do this for ease of analysis and because many of the models we use will not consider a row that includes NA's in any of the columns being used for the model.  

# Exploratory Data Analysis

Before we dig too deeply in to the dataset, it is important to understand its structure: 
```{r}
# Number of rows
nrow(gss_subset) 

# Number of columns
ncol(gss_subset) 

# Response variable summary
summary(gss_subset$partyid)

# Data Structure
str(gss_subset)

# Glimpse of dataset
gss_subset %>%
  select(-weight) %>%
  rename(home = hompop, party = partyid) %>%
  head() %>%
  knitr::kable()
```

As we first explore the dataset, we can look at the distribution of democrats and republications in our dataset in counts:
```{r partyid-dist, echo = FALSE}
ggplot(gss_subset, aes(x = partyid)) +
  geom_bar(fill = c("steelblue", "tomato"),
           color = "black") +
  labs(title = "Count of Democrat and Republican Party Affiliation in our Dataset")
```

There appears to me more democrats than republicans represented in this dataset, which could be because democrats are more likely to participate in this survey, or it could be that the way we selected our data systemically oversampled democrats. Notably from this, it is the case that our the weights associated with our sample of the GSS dataset would not be the same as the weights that the GSS uses for the dataset, so the `weight` variable should be ignored entirely. 

Now, we can examine some of our predictor variables with our response, `partyid`, to see the relationships there are between variables. First, we see in this side-by-side boxplot with the means plotted on top that republicans tend to be older on average:
```{r age-plot, echo = FALSE, warning = FALSE, message = FALSE}
ggplot(gss_subset, aes(x = age,
                       y = partyid,
                       color = partyid)) +
  geom_boxplot(alpha = 0.3) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "bottom") +
  scale_color_manual(values = c("steelblue", "tomato")) +
  stat_summary(fun = mean, size = 0.25) +
  labs(title = "Political Affiliation by Age",
       x = "Age",
       y = "Political Affiliation",
       color = "Political Affiliation")
```

Next, it is interesting to consider economic status across political affiliations. By comparing political affiliation to income, class, and hours worked in the last week we can see small relationships between political affiliation and economic status:
```{r economic-status, echo = FALSE, warning = FALSE, message = FALSE}
ggplot(gss_subset, aes(x = income,
                       y = partyid,
                       color = partyid)) +
  geom_jitter(alpha = 0.5) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "bottom") +
  scale_color_manual(values = c("steelblue", "tomato")) +
  labs(
    title = "Political Affiliation by Income",
    x = "Income",
    y = "Political Affiliation",
    color = "Political Affiliation"
  )

ggplot(gss_subset, aes(x = class,
                       y = partyid,
                       color = partyid)) +
  geom_jitter(alpha = 0.3) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "bottom") +
  scale_color_manual(values = c("steelblue", "tomato")) +
  labs(
    title = "Political Affiliation by Class",
    x = "Class",
    y = "Political Affiliation",
    color = "Political Affiliation"
  )

ggplot(gss_subset, aes(x = hours,
                       y = partyid,
                       color = partyid)) +
  geom_boxplot(alpha = 0.3) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "bottom") +
  scale_color_manual(values = c("steelblue", "tomato")) +
  labs(
    title = "Political Affiliation by Hours Worked Last Week",
    subtitle = "Average plotted with point on plot",
    x = "Hours Worked Last Week",
    y = "Political Affiliation",
    color = "Political Affiliation"
  ) +
  stat_summary(fun = mean, size = 0.25)
```

It is also relevant to look at other variables such as race, marital status, and education as factors related to political party affiliation. Most notably, there is a much larger proportion of white republicans than democrats. We can see this in the first plot in the following plots: 
```{r, echo = FALSE, message = FALSE, warning = FALSE}
ggplot(gss_subset, aes(x = race,
                       y = partyid,
                       color = partyid)) +
  geom_jitter(alpha = 0.3) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "bottom") +
  scale_color_manual(values = c("steelblue", "tomato")) +
  labs(
    title = "Political Affiliation by Race",
    x = "Race",
    y = "Political Affiliation",
    color = "Political Affiliation"
  )

ggplot(gss_subset, aes(x = marital,
                       y = partyid,
                       color = partyid)) +
  geom_jitter(alpha = 0.3) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "bottom") +
  scale_color_manual(values = c("steelblue", "tomato")) +
  labs(
    title = "Political Affiliation by Martial Status",
    x = "Marital Status",
    y = "Political Affiliation",
    color = "Political Affiliation"
  )


reps <- gss_subset %>%
  filter(partyid == "rep")
dems <- gss_subset %>%
  filter(partyid == "dem")
ggplot() +
  geom_point(reps,
             mapping = aes(x = marital,
                           y = age,
                           color = partyid),
             position = position_nudge(x = -0.1),
             alpha = 0.5) +
  geom_point(dems,
             mapping = aes(x = marital,
                           y = age,
                           color = partyid),
             position = position_nudge(x = 0.1),
             alpha = 0.5) +
  scale_color_manual(values = c("steelblue", "tomato")) +
  theme(legend.position = "bottom") +
  labs(
    title = "Marital Status by Age",
    subtitle = "Colored by political affiliation",
    x = "Marital Status",
    y = "Age",
    color = "Political Affiliation"
  )

ggplot(gss_subset, aes(x = as.numeric(educ),
                       y = partyid,
                       color = partyid)) +
  geom_boxplot(alpha = 0.3)+
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "bottom") +
  scale_color_manual(values = c("steelblue", "tomato")) +
  labs(
    title = "Political Affiliation by Years of Education",
    subtitle = "Average plotted with point on plot",
    x = "Educational Years Completed",
    y = "Political Affiliation",
    color = "Political Affiliation"
  ) +
  stat_summary(fun = mean, size = 0.25)
```

After completing these exploratory analyses, it is clear that while there are some weak relationships within many variables, we will likely need all of these variables to make models which have good predictive power. None of the predictors appear to have an extremely strong relationship with political party affiliation, and so we will need to use many of them for our models to perform well. 

We also examined two classification model methods for accuracy in predicting partyid based on some of our 16 predictors. Linear disriminany analysis (LDA) appeare to perform better job correctly classifying Democrats than Republicans based on these 6 predictors, as there was an equal amount of Republicans incorrectly predicted to those correctly predicted. Our logistic regression model with all 16 predictors also appears to better classify Democrats than Republicans, but not by much, with an overall training error rate of about 18%. These results suggest that the current classification models we have used throughout this course may not be successful in predicting partyid with high accuracy on their own. We hope to leverage these methods through model stacking in our Methods and Results section.
```{r}
#taking a look at how LDA could perform on our dataset with just a couple of variables 
library(MASS)
set.seed(2020)
mlda <- lda(partyid ~ race + age + year + hompop + income + wrkgovt, data = gss_subset)
mlda_pred <- predict(mlda)
conf_mlda <- table(mlda_pred$class,gss_subset$partyid)
conf_mlda

#taking a look at how logistic regression could perform on our dataset with just a couple of variables and then full model 
simple_logreg<-glm(partyid ~ race + age + year + hompop + income + wrkgovt, data = gss_subset, family= "binomial")
summary(simple_logreg)

full_logreg<-glm(partyid ~ ., data = gss_subset, family= "binomial")
summary(full_logreg)

probs<-predict(full_logreg, gss_subset, type = "response")
preds<-ifelse(probs >=.5, 1, 0)
conf_log <- table(preds, gss_subset$partyid)
conf_log

n <- length(gss_subset$partyid)
false_pos <- conf_log[1,2]
false_neg <- conf_log[2,1]
error <- 1/n *(false_pos + false_neg)
error
```

# Results 

# Discussion

# Code Appendix



# References